{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cdd8e60-4cbb-4fe2-96e3-ef2f2bfdf642",
   "metadata": {},
   "source": [
    "<br>\n",
    "<img src=\"pics/logo_hct.png\" alt=\"Figure 1\" style=\"width: 300px;\"/>\n",
    "<!-- <h4 style=\"text-align: center;\" markdown=\"1\">  Figure 2c from (Ferreira et al. 2014). Note that the colorbar legend at the bottom as well as the arrows are not required in the assignment descriptions below.</h4> -->\n",
    "<br>\n",
    "\n",
    "**Elaborado por**: Pérez-Estrada, Adolfo & Peña-Mata, Claudia Elizabeth \n",
    "\n",
    "**Fecha de elaboración**: 14 de mayo de 2024\n",
    "\n",
    "# CLASE 1. METODOS DE APRENDIZAJE AUTOMATICO (*MACHINE LEARNING*): Regresión lineal\n",
    "\n",
    "## Introducción de Actividad Práctica: Modelado de Regresión con Datos Meteorológicos\n",
    "\n",
    "Bienvenidos a esta actividad práctica donde exploraremos cómo aplicar modelos de regresión lineal a un conjunto de datos meteorológicos. En este notebook, nuestro objetivo será construir un modelo predictivo de **REGRESIÓN LINEAL**, utilizando variables históricas. Para hacerlo, pasaremos por varias etapas cruciales en el proceso de modelado estadístico y aprendizaje automático. A continuación, detallo los pasos que seguiremos:\n",
    "\n",
    "### 1. Preparación del Entorno\n",
    "Antes de comenzar con el análisis y modelado de datos, es esencial preparar nuestro entorno de trabajo. Esto incluye la importación de librerías y módulos necesarios para la manipulación de datos, visualización y modelado estadístico. Algunas de las librerías que utilizaremos son `pandas` para la gestión de datos, `matplotlib` y `seaborn` para visualización, y `scikit-learn` para crear y evaluar modelos de regresión.\n",
    "\n",
    "### 2. Adquisición de Datos\n",
    "Los datos son el núcleo de cualquier análisis estadístico. En esta sección, aprenderemos cómo descargar los datos meteorológicos que necesitamos. Discutiremos las fuentes de estos datos y cómo asegurarnos de que la descarga sea exitosa y segura, utilizando herramientas y técnicas adecuadas para la manipulación de datos en línea.\n",
    "\n",
    "### 3. Preprocesamiento de Datos\n",
    "Una vez que tenemos los datos, el siguiente paso es prepararlos para el análisis. Esto incluye limpiar los datos de posibles errores, llenar o eliminar valores faltantes, y realizar transformaciones necesarias para el análisis. Esta etapa es crucial para asegurar la calidad de los resultados del modelo.\n",
    "\n",
    "### 4. División del Conjunto de Datos\n",
    "Para evaluar la efectividad de nuestro modelo de regresión, dividiremos nuestros datos en un conjunto de entrenamiento y otro de prueba. Esto nos permite entrenar nuestro modelo en un subconjunto de datos y luego probarlo en datos que no ha visto antes, proporcionando una evaluación más robusta de su desempeño.\n",
    "\n",
    "### 5. Aplicación de la Regresión Lineal\n",
    "Con los datos preparados y divididos, procederemos a aplicar el modelo de regresión lineal. Explicaremos la teoría detrás de la regresión lineal, cómo funciona, y cómo puede ser aplicada a nuestros datos meteorológicos. Configuraremos nuestro modelo, lo entrenaremos con el conjunto de entrenamiento y luego lo utilizaremos para hacer predicciones sobre el conjunto de prueba.\n",
    "\n",
    "### 6. Evaluación del Modelo\n",
    "Finalmente, evaluaremos el desempeño de nuestro modelo utilizando métricas estándar como el Error Absoluto Medio (MAE), el Error Cuadrático Medio (MSE) y el coeficiente de determinación (R2). Estas métricas nos proporcionarán una comprensión clara de cuán bien nuestro modelo está realizando predicciones basadas en los datos de prueba.\n",
    "\n",
    "Al final de este notebook, no solo habrás aprendido a aplicar un modelo de regresión lineal, sino también a entender y evaluar su rendimiento en un contexto real de datos meteorológicos. Este conocimiento es esencial para cualquier aspirante a científico de datos o analista en el campo de la meteorología y más allá.\n",
    "\n",
    "Preparemos nuestro entorno, carguemos los datos, y sumérgete en el fascinante mundo de la modelación estadística. ¡Comencemos!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eac295-22ea-48c2-95cc-b1bc0f9938d1",
   "metadata": {},
   "source": [
    "## 1. Preparación del entorno \n",
    "\n",
    "Para este notebook serán necesarias las siguientes librerías: \n",
    "\n",
    "1. **Pandas (1.3.4)**:\n",
    "   - **Utilidad**: Es una biblioteca de Python especializada en la manipulación y el análisis de datos. Ofrece estructuras de datos y operaciones para manipular tablas numéricas y series temporales. Es indispensable para la limpieza, transformación, y análisis de datos.\n",
    "\n",
    "2. **NumPy (1.21.4)**:\n",
    "   - **Utilidad**: Esencial para la computación científica con Python. Proporciona un objeto de arreglo multidimensional, diversos objetos derivados (como matrices enmascaradas y matrices), y una variedad de rutinas para operaciones rápidas en arreglos, incluyendo matemáticas, lógica, manipulación de formas, ordenación, selección, I/O, transformadas discretas de Fourier, álgebra lineal básica, operaciones estadísticas básicas, simulación aleatoria y mucho más.\n",
    "\n",
    "3. **Seaborn (0.9.0)**:\n",
    "   - **Utilidad**: Es una biblioteca de visualización de datos en Python basada en matplotlib. Proporciona una interfaz de alto nivel para la creación de gráficos estadísticos atractivos e informativos. Es particularmente útil para explorar y entender datos complejos a través de visualizaciones más sofisticadas y temáticas.\n",
    "\n",
    "4. **Matplotlib (3.5.0)**:\n",
    "   - **Utilidad**: Es una biblioteca de trazado para la creación de visualizaciones estáticas, animadas e interactivas en Python. Ofrece una gran variedad de gráficos y diagramas, lo que permite a los usuarios visualizar datos de formas muy variadas y personalizadas. Es la base sobre la que se construyen muchas otras bibliotecas de visualización, incluida Seaborn.\n",
    "\n",
    "Estas bibliotecas son herramientas fundamentales en el ámbito del análisis de datos y la ciencia de datos, permitiendo desde la manipulación de datos hasta la creación de visualizaciones complejas para interpretaciones más profundas y presentaciones efectivas.\n",
    "\n",
    "---\n",
    "\n",
    "NOTA: \n",
    "Importación de las bibliotecas requeridas para este laboratorio. Descomentar si no se tienen las librerias instaladas.\n",
    "\n",
    "Si tu entorno no admite el uso de \"!mamba install\", utiliza \"!pip install\" en su lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a2834bc-c52b-441c-8c54-32bcfcc4a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d5487-1d07-4db8-bfd0-bb5af5b349bc",
   "metadata": {},
   "source": [
    "5. **Scikit-learn**:\n",
    "\n",
    "Es una biblioteca de aprendizaje automático (machine learning) para Python. Proporciona una gama de herramientas supervisadas y no supervisadas a través de una interfaz consistente en Python. Scikit-learn es ampliamente utilizada para diversas aplicaciones de machine learning, incluyendo clasificación, regresión, agrupamiento y reducción de dimensionalidad.\n",
    "\n",
    "##### Por qué se utiliza:\n",
    "\n",
    "Scikit-learn se utiliza principalmente porque simplifica la implementación de algoritmos de machine learning, lo que permite a los usuarios y desarrolladores concentrarse en la formulación de sus problemas más que en la programación de algoritmos desde cero. Además, al ser un proyecto de código abierto, permite la colaboración y mejora continua por parte de la comunidad de machine learning, asegurando que las técnicas implementadas estén al día con los avances más recientes en el campo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccfdda54-7f8b-403a-97bd-a806b3e6c9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\adolf\\miniconda3\\envs\\ml_project\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\adolf\\miniconda3\\envs\\ml_project\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\adolf\\miniconda3\\envs\\ml_project\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\adolf\\miniconda3\\envs\\ml_project\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\adolf\\miniconda3\\envs\\ml_project\\lib\\site-packages (from scikit-learn) (3.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409df9e-eb43-4823-b091-49e94895a720",
   "metadata": {},
   "source": [
    "Función para **suprimir errores o sugerencias (***warnings***)** de ejecución de las librerías: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f605edc-21c9-4d1c-be25-a520eda09f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcb3ff4-bd07-497e-9e64-2c7c8a308df5",
   "metadata": {},
   "source": [
    "### Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9499e9-c751-48ab-abbf-1d20b825fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de bibliotecas para manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importación de modelos de machine learning de scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "# Importación de herramientas de preprocesamiento de scikit-learn\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Importación de funciones de división de datos y evaluación de modelos\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import jaccard_score, f1_score, log_loss\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b169a-186d-4dea-98f0-d4a46bea934a",
   "metadata": {},
   "source": [
    "## 2. ADQUISICIÓN DE DATOS\n",
    "\n",
    "La fuente original de los datos es el Bureau of Meteorology del Gobierno de Australia, y los datos más recientes se pueden obtener desde [http://www.bom.gov.au/climate/dwo/](http://www.bom.gov.au/climate/dwo/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2022-01-01).\n",
    "\n",
    "<br>\n",
    "<img src=\"pics/BOM.png\" alt=\"Figure 2\" style=\"width: 300px;\"/>\n",
    "\n",
    "El conjunto de datos que se utilizará tiene columnas adicionales como 'RainToday' y nuestro objetivo es 'RainTomorrow', que se obtuvo de Rattle en [https://bitbucket.org/kayontoga/rattle/src/master/data/weatherAUS.RData](https://bitbucket.org/kayontoga/rattle/src/master/data/weatherAUS.RData?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2022-01-01).\n",
    "\n",
    "Contiene observaciones de métricas meteorológicas para cada día desde 2008 hasta 2017.\n",
    "\n",
    "El conjunto de datos **weatherAUS.csv** incluye los siguientes campos:\n",
    "\n",
    "\n",
    "| Campo           | Descripción                                           | Unidad            | Tipo   |\n",
    "| --------------- | ----------------------------------------------------- | ---------------- | ------ |\n",
    "| Date            | Fecha de la observación en YYYY-MM-DD                 | Fecha            | object |\n",
    "| Location        | Ubicación de la observación                           | Ubicación        | object |\n",
    "| MinTemp         | Temperatura mínima                                    | Celsius          | float  |\n",
    "| MaxTemp         | Temperatura máxima                                    | Celsius          | float  |\n",
    "| Rainfall        | Cantidad de precipitación                             | Milímetros       | float  |\n",
    "| Evaporation     | Cantidad de evaporación                               | Milímetros       | float  |\n",
    "| Sunshine        | Cantidad de luz solar directa                         | Horas            | float  |\n",
    "| WindGustDir     | Dirección de la ráfaga más fuerte                     | Puntos cardinales| object |\n",
    "| WindGustSpeed   | Velocidad de la ráfaga más fuerte                     | Kilómetros/Hora  | object |\n",
    "| WindDir9am      | Dirección del viento promediada 10 minutos antes de las 9am | Puntos cardinales| object |\n",
    "| WindDir3pm      | Dirección del viento promediada 10 minutos antes de las 3pm | Puntos cardinales| object |\n",
    "| WindSpeed9am    | Velocidad del viento promediada 10 minutos antes de las 9am | Kilómetros/Hora  | float  |\n",
    "| WindSpeed3pm    | Velocidad del viento promediada 10 minutos antes de las 3pm | Kilómetros/Hora  | float  |\n",
    "| Humidity9am     | Humedad a las 9am                                     | Porcentaje       | float  |\n",
    "| Humidity3pm     | Humedad a las 3pm                                     | Porcentaje       | float  |\n",
    "| Pressure9am     | Presión atmosférica reducida al nivel medio del mar a las 9am | Hectopascal     | float  |\n",
    "| Pressure3pm     | Presión atmosférica reducida al nivel medio del mar a las 3pm | Hectopascal     | float  |\n",
    "| Cloud9am        | Fracción del cielo oscurecida por nubes a las 9am     | Octavos          | float  |\n",
    "| Cloud3pm        | Fracción del cielo oscurecida por nubes a las 3pm     | Octavos          | float  |\n",
    "| Temp9am         | Temperatura a las 9am                                 | Celsius          | float  |\n",
    "| Temp3pm         | Temperatura a las 3pm                                 | Celsius          | float  |\n",
    "| RainToday       | Si llovió hoy                                         | Sí/No            | object |\n",
    "| RainTomorrow    | Si lloverá mañana                                     | Sí/No            | float  |\n",
    "\n",
    "Las definiciones de las columnas se recopilaron de [http://www.bom.gov.au/climate/dwo/IDCJDW0000.shtml](http://www.bom.gov.au/climate/dwo/IDCJDW0000.shtml?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2022-01-01)\n",
    "\n",
    "**Cargamos los datos leyendo desde la función de lectura de PANDAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510796ea-0b9c-4b39-b8ef-f55cbd07d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Weather_Data.csv\")\n",
    "df.head() \n",
    "## Por default la función head traerá los primeras 5 filas de datos, cambiala para observar las primeras 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ce0df-1c01-4d65-8ac5-5883936deabf",
   "metadata": {},
   "source": [
    "##### EJERCICIO: ¿Que función utilizarias para ver los últimos registros del dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3063400e-92b5-4ef3-89d3-9a4b10b8f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribe aquí tu sugerencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087aa290-14ce-42fe-8273-2a347e337312",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento de Datos\n",
    "El primer paso en el preprocesamiento de datos para este análisis implica una exploración inicial y comprensiva de las medidas de tendencia central y dispersión de los datos. Este proceso nos permite obtener una visión general y detallada de las características estadísticas básicas del conjunto de datos, facilitando la identificación de patrones, valores atípicos y errores potenciales que podrían afectar el rendimiento del modelo de regresión. A continuación, se describen las actividades a realizar en este paso:\n",
    "\n",
    "1. **Uso de la función `describe()` de Pandas**: \n",
    "   - Esta función proporciona un resumen estadístico rápido y útil de cada columna numérica en el DataFrame. Esto incluye la media, la mediana (50%), la desviación estándar, los valores mínimos y máximos, así como los percentiles 25% y 75%.\n",
    "   - Para ejecutarlo, simplemente utilizamos `dataframe.describe()` donde `dataframe` es nuestro conjunto de datos. Es importante asegurarse de que todas las columnas que se desean incluir sean de tipo numérico; de lo contrario, serán excluidas del resumen.\n",
    "\n",
    "2. **Exploración de estadigrafos con función calculada**:\n",
    "    - Para expandir el análisis vamos a ejecutar una función creada para obtener los valores similares a la función de pandas con la adición de valores como cuartiles, rango intercuartilicos, sesgo y curtosis.\n",
    "      \n",
    "3. **Visualización de las distribuciones de datos**:\n",
    "   - Complementaremos el análisis de la tabla `describe()` con visualizaciones gráficas para una mejor interpretación y comprensión de la distribución de los datos. Las visualizaciones como histogramas, gráficos de caja (box plots) y gráficos de violín son especialmente útiles para este propósito.\n",
    "   - **Histogramas**: Ayudan a ver la forma de la distribución de los datos, mostrando la frecuencia de valores en intervalos específicos. Son excelentes para detectar la simetría y sesgos en los datos.\n",
    "   - **Gráficos de caja (Box plots)**: Ofrecen una representación visual de cómo están distribuidos los datos en términos de cuartiles y ayudan a identificar claramente los valores atípicos.\n",
    "\n",
    "4. **Preprocesamiento específico**\n",
    "\n",
    "La combinación de métodos estadísticos descriptivos y visualizaciones gráficas en este primer paso es crucial para establecer un entendimiento sólido del conjunto de datos antes de proceder a pasos más avanzados de preprocesamiento y modelado. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc379bb2-c57e-4958-83ee-89adc71ffa52",
   "metadata": {},
   "source": [
    "### 1. Uso de la función `describe()` de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2734d373-4432-4b9f-8db9-a2f2afbe7840",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b798eb48-b0c7-4465-9c87-b673d59a7898",
   "metadata": {},
   "source": [
    "##### Describe el comportamiento de las variable `Rainfall`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5726e84b-27b7-4f9c-a345-0b7454a81941",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66e769ba-1cc9-46ea-a859-ebf2da0deb04",
   "metadata": {},
   "source": [
    "### 2. Exploración de estadigrafos con función calculada `estadigrafos()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaa71c4-01bc-41d6-a2e2-94ac5a793c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estadigrafos(df, VariableName):\n",
    "    \"\"\"\n",
    "    Calcula estadísticas descriptivas básicas para una columna específica de un DataFrame.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pandas.DataFrame): DataFrame que contiene los datos.\n",
    "    - VariableName (str): Nombre de la columna para la cual calcular las estadísticas.\n",
    "    \n",
    "    Retorna:\n",
    "    - pandas.DataFrame: DataFrame con las estadísticas de la columna especificada.\n",
    "    \"\"\"\n",
    "    # Calcula las estadísticas deseadas\n",
    "    minim = df[VariableName].min()\n",
    "    Q1 = df[VariableName].quantile(0.25)\n",
    "    prome = df[VariableName].mean()\n",
    "    Q2 = df[VariableName].median()\n",
    "    Q3 = df[VariableName].quantile(0.75)\n",
    "    maxim = df[VariableName].max()\n",
    "    varia = df[VariableName].var()\n",
    "    desvi = df[VariableName].std()\n",
    "    rango = maxim - minim\n",
    "    IQR = Q3 - Q1\n",
    "    coefv = desvi / prome if prome != 0 else np.nan  # Previene división por cero\n",
    "    simetria = df[VariableName].skew() \n",
    "    curtosis = df[VariableName].kurtosis()\n",
    "    \n",
    "    # Crea un DataFrame con los resultados\n",
    "    estadisticas = pd.DataFrame({\n",
    "        VariableName: [minim, Q1, prome, Q2, Q3, maxim, varia, desvi, rango, IQR, coefv, simetria, curtosis]\n",
    "    }, index=['Mínimo', 'Primer Cuartil', 'Promedio', 'Mediana', 'Tercer Cuartil', 'Máximo', 'Varianza', 'Desviación Estandar', 'Rango', 'IQR', 'Coeficiente de variación', 'Simetria', 'Curtosis'])\n",
    "    \n",
    "    return estadisticas.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe971cab-4085-4472-abf4-d25105fe972c",
   "metadata": {},
   "source": [
    "La función anterior requiere dos variables de entrada para su ejecución `df` y `VariableName`. La primera será el dataframe que contiene los datos de análisis. La segunda variable requiere los nombres de las variables que quieres analizar. \n",
    "\n",
    "Como en este caso se ejecuta por variable, vamos a construir un código que itere el nombre de las variables para crear una tabla visualmente similar a la que se ve en la función de pandas `pandas.DataFrame.describe()`.\n",
    "\n",
    "Para obtenerlo podemos: \n",
    "- Escribir manualmente el nombre de las columnas. \n",
    "- Usar la función `pandas.DataFrame.columns()`\n",
    "\n",
    "Implementaremos la segunda opción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ea697-1ba5-4be6-85f5-dfc19483cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_a_analizar = df.columns()\n",
    "columnas_a_analizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d637a-ba52-41f3-a8af-7c76e8e10444",
   "metadata": {},
   "source": [
    "Para continuar hacemos la iteración de las columnas con un ciclo `for`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b51eb-aec6-4732-8b42-505eedd3a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "estadisticos = pd.DataFrame()\n",
    "for variable in columnas_a_analizar:\n",
    "    df_est = estadigrafos(df, variable)\n",
    "    estadisticos = pd.concat([estadisticos, df_est], axis = 1)\n",
    "\n",
    "estadisticos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e054c29b-7ef6-493d-abfb-95294e88a373",
   "metadata": {},
   "source": [
    "### 3. Visualización de las distribuciones de datos\n",
    "\n",
    "Para crear una función en Python que genere una gráfica por variable de interés, donde se puede especificar el nombre de la variable y el DataFrame como parámetros de entrada, se puede modificar y simplificar el código proporcionado. A continuación, te muestro cómo hacerlo paso a paso, enfocándonos en generar una sola gráfica para una variable específica cada vez que se llama a la función:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_variable(dataframe, variable_name):\n",
    "    \"\"\"\n",
    "    Función para graficar la distribución de una variable única junto con estadísticas importantes.\n",
    "\n",
    "    Parámetros:\n",
    "    - dataframe (pd.DataFrame): DataFrame de donde se extraerá la variable.\n",
    "    - variable_name (str): Nombre de la columna del DataFrame a graficar.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "La función `plot_variable` toma dos argumentos:\n",
    "- `dataframe`: El DataFrame de Pandas que contiene los datos.\n",
    "- `variable_name`: El nombre de la columna que se desea graficar.\n",
    "\n",
    "### Funcionalidad de la Función:\n",
    "- Configura el estilo de visualización usando Seaborn.\n",
    "- Crea una figura y un eje para la gráfica.\n",
    "- Calcula y muestra en la gráfica las estadísticas principales de la variable (media, mediana, moda, máximo y mínimo) utilizando líneas verticales de diferentes colores.\n",
    "- Añade una leyenda para identificar cada estadística.\n",
    "\n",
    "Esta función es útil para realizar un análisis visual rápido y detallado de una variable específica dentro de un conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f15a7e3-aefe-4f43-81f5-34991b883404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_variable(dataframe, variable_name):\n",
    "    \"\"\"\n",
    "    Función para graficar la distribución de una variable única junto con estadísticas importantes.\n",
    "\n",
    "    Parámetros:\n",
    "    - dataframe (pd.DataFrame): DataFrame de donde se extraerá la variable.\n",
    "    - variable_name (str): Nombre de la columna del DataFrame a graficar.\n",
    "    \"\"\"\n",
    "    # Ajustes de visualización con Seaborn\n",
    "    custom_params = {\"axes.spines.right\": True, \"axes.spines.top\": True, \"axes.spines.left\": True}\n",
    "    sns.set_theme(style='whitegrid', context=\"talk\", rc=custom_params, color_codes=True)\n",
    "    sns.set_style(\"ticks\", {\"axes.grid\": True, \"grid.linestyle\": \"--\"})\n",
    "    \n",
    "    # Crea la figura\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Crea el gráfico para la variable especificada\n",
    "    ax = sns.histplot(dataframe[variable_name], bins=10, kde=True, color='yellowgreen', legend=False)\n",
    "    ax.set_title(f'Distribución de {variable_name}', fontsize=14)\n",
    "    ax.set_ylabel('Frecuencia', fontsize=12)\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    # Calcula estadísticas\n",
    "    mean = dataframe[variable_name].mean()\n",
    "    median = dataframe[variable_name].median()\n",
    "    mode = dataframe[variable_name].mode()[0]\n",
    "    max_value = dataframe[variable_name].max()\n",
    "    min_value = dataframe[variable_name].min()\n",
    "    \n",
    "    # Agrega líneas verticales para cada estadística\n",
    "    lines = [\n",
    "        ax.axvline(mean, color='red', linestyle='dashed', linewidth=2, label=f'Media: {mean:.2f}'),\n",
    "        ax.axvline(median, color='green', linestyle='dashed', linewidth=2, label=f'Mediana: {median:.2f}'),\n",
    "        ax.axvline(mode, color='blue', linestyle='dashed', linewidth=2, label=f'Moda: {mode}'),\n",
    "        ax.axvline(max_value, color='purple', linestyle='dashed', linewidth=2, label=f'Máx: {max_value:.2f}'),\n",
    "        ax.axvline(min_value, color='orange', linestyle='dashed', linewidth=2, label=f'Mín: {min_value:.2f}')\n",
    "    ]\n",
    "    \n",
    "    # Añade la leyenda\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    \n",
    "    # Muestra la gráfica\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb139900-0f11-4a63-812d-7ebca260d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variable(df, 'Rainfall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03412cde-9823-4acd-9182-44d6434f18c9",
   "metadata": {},
   "source": [
    "### 4. Preprocesamiento específico\n",
    "\n",
    "Necesitamos realizar una codificación \"one hot\" para convertir variables categóricas en variables binarias.\n",
    "\n",
    "#### ¿Qué es la codificación one hot?\n",
    "\n",
    "La codificación one hot transforma cada categoría de una variable categórica en una nueva variable binaria (0 o 1). Cada una de estas nuevas variables binarias representa la presencia (o ausencia) de una categoría específica. Esto es particularmente útil en modelos de aprendizaje automático, ya que muchos algoritmos no pueden manejar directamente las variables categóricas y requieren que los datos de entrada estén en formatos numéricos.\n",
    "\n",
    "#### ¿Cómo funciona la codificación one hot?\n",
    "\n",
    "Supongamos que tenemos una variable categórica llamada \"Color\" con tres categorías: Rojo, Verde y Azul. La codificación one hot convierte esta variable en tres nuevas variables: una para cada categoría. Así es como se vería:\n",
    "\n",
    "- **Original:**\n",
    "  - Color: Rojo\n",
    "  - Color: Verde\n",
    "  - Color: Azul\n",
    "\n",
    "- **Codificado One Hot:**\n",
    "  - Color_Rojo: 1, Color_Verde: 0, Color_Azul: 0\n",
    "  - Color_Rojo: 0, Color_Verde: 1, Color_Azul: 0\n",
    "  - Color_Rojo: 0, Color_Verde: 0, Color_Azul: 1\n",
    "\n",
    "#### Beneficios de la codificación one hot\n",
    "\n",
    "1. **Interpretación del Modelo:** Facilita la interpretación de los resultados en modelos estadísticos y de machine learning al eliminar el orden ordinal implícito que puede tener una codificación numérica directa (por ejemplo, 1 para Rojo, 2 para Verde, 3 para Azul), lo cual puede ser incorrecto si las categorías no tienen un orden natural.\n",
    "   \n",
    "2. **Compatibilidad con Algoritmos:** Muchos algoritmos de aprendizaje automático esperan datos de entrada numéricos, por lo que la codificación one hot permite la integración directa de atributos categóricos en estos modelos.\n",
    "\n",
    "#### Implementación en Python\n",
    "\n",
    "En Python, la codificación one hot puede realizarse fácilmente utilizando bibliotecas como `pandas` o `scikit-learn`. Con `pandas`, se puede usar la función `get_dummies()`, mientras que `scikit-learn` ofrece `OneHotEncoder` en su módulo `preprocessing`.\n",
    "\n",
    "Este proceso de codificación es fundamental para preparar tus datos para modelos de aprendizaje automático y debe ser comprendido y utilizado adecuadamente para obtener los mejores resultados de tus análisis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532c48b-31e5-4ef5-b837-a743f2c4075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sydney_processed = pd.get_dummies(data=df, columns=['RainToday', 'WindGustDir', 'WindDir9am', 'WindDir3pm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ec465-d71a-4c74-9900-9d8daf500c97",
   "metadata": {},
   "source": [
    "A continuación, reemplazamos los valores de la columna `RainTomorrow` cambiándolos de una columna categórica a una columna binaria. No utilizamos el método get_dummies porque terminaríamos con dos columnas para `RainTomorrow`, lo cual no deseamos, ya que `RainTomorrow` es nuestro objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8bea19-8ed6-4b6f-9316-98237bab719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sydney_processed.replace(['No', 'Yes'], [0,1], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416607a4-8804-4832-927a-c6f1fe821df8",
   "metadata": {},
   "source": [
    "## 4. División del Conjunto de Datos\n",
    "Ahora, establecemos nuestras 'variables' o valores de $x$ y nuestra $y$ o variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612e12b-efbd-4ff2-bed3-e4cf0af2de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos aquellos datos que NO sean númericos\n",
    "df_sydney_processed.drop('Date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d52f227-c262-4341-ab64-40a2c349435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos aseguramos que los datos sean de tipo númerico\n",
    "df_sydney_processed = df_sydney_processed.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f9952-d056-4028-b482-c34cda760418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el dataframe en conjunto de datos para entrar a los modelos\n",
    "features = df_sydney_processed.drop(columns='RainTomorrow', axis=1)\n",
    "Y = df_sydney_processed['RainTomorrow']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df47b706-4318-4112-8bf0-44964dd52389",
   "metadata": {},
   "source": [
    "## 5. Aplicación de la Regresión Lineal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43d0279-5898-4f78-a612-5954d4ba116b",
   "metadata": {},
   "source": [
    "#### A1) Utiliza la función `train_test_split` para dividir los dataframes `features` y `Y` con un `test_size` de `0.2` y el `random_state` establecido en `10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe38608-6d66-41d3-9307-b3f0f967c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features, Y, random_state=10, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f7137-43df-451f-a59c-befc5b1d0006",
   "metadata": {},
   "source": [
    "Esta línea de código utiliza la función `train_test_split` del módulo `sklearn.model_selection` para dividir dos conjuntos de datos —`features` y `Y`— en subconjuntos de entrenamiento y prueba. Aquí está el detalle de cada parte:\n",
    "\n",
    "1. **`features`**: Es el DataFrame o array que contiene las variables independientes o predictores que se utilizarán para predecir la variable objetivo. Este conjunto incluye todas las características (por ejemplo, temperatura, humedad) que el modelo usará para aprender.\n",
    "\n",
    "2. **`Y`**: Es el DataFrame o array que contiene la variable dependiente o objetivo que se está tratando de predecir. En el contexto de un problema de clasificación, por ejemplo, podría ser si lloverá mañana o no.\n",
    "\n",
    "3. **`random_state=10`**: Es un parámetro que sirve para asegurar la reproducibilidad de los resultados. Al especificar un número (en este caso, 10), el algoritmo de división siempre generará el mismo resultado cada vez que se ejecute el código, siempre que los datos de entrada no cambien. Esto es útil para la depuración y para comparar el desempeño del modelo bajo condiciones consistentes.\n",
    "\n",
    "4. **`test_size=.2`**: Este parámetro especifica la proporción del conjunto de datos que se separará como conjunto de prueba. En este caso, el 20% de los datos será usado para el conjunto de prueba, mientras que el restante 80% será usado para el conjunto de entrenamiento. El conjunto de prueba es el que se utiliza para evaluar el rendimiento del modelo después de entrenarlo con el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91093f-f5e6-458c-9dae-33a5b019d154",
   "metadata": {},
   "source": [
    "#### A2. Crea y entrena un modelo de Regresión Lineal llamado LinearReg usando los datos de entrenamiento `(x_train, y_train)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17cf9cf-0f74-4682-ba3c-7659850dcf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearReg = LinearRegression()\n",
    "LinearReg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7e0e9d-0381-4c7f-8904-6b8f26572a6c",
   "metadata": {},
   "source": [
    "1. **`LinearReg = LinearRegression()`**:\n",
    "   - Esta línea de código inicializa un nuevo modelo de regresión lineal. `LinearRegression()` es una función de `scikit-learn` que crea un objeto de modelo de regresión lineal. Al llamar a esta función, se configuran todos los parámetros predeterminados necesarios para el modelo.\n",
    "   - `LinearReg` es el nombre que le asignamos a este objeto de modelo. Es un identificador que usaremos para acceder a las funciones del modelo, como el entrenamiento y la predicción.\n",
    "\n",
    "2. **`LinearReg.fit(x_train, y_train)`**:\n",
    "   - Esta es la línea donde efectivamente se entrena el modelo de regresión lineal. La función `fit()` es un método proporcionado por `scikit-learn` que toma los datos de entrada (`x_train`) y las etiquetas o respuestas asociadas (`y_train`) para entrenar el modelo.\n",
    "   - `x_train` contiene las características o variables independientes de los datos de entrenamiento, que el modelo usará para aprender.\n",
    "   - `y_train` contiene la variable objetivo o dependiente correspondiente a `x_train`, que es lo que el modelo intenta predecir.\n",
    "   - Durante el entrenamiento, el algoritmo de regresión lineal ajusta los mejores parámetros (coeficientes e intercepción) para minimizar el error entre las predicciones hechas con el modelo y los valores reales en `y_train`. Esto se logra a menudo a través de un método matemático como el descenso de gradiente o la ecuación normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fbac6a-3b49-4d4e-9ee4-d8f99ba12a4b",
   "metadata": {},
   "source": [
    "#### A3) Ahora utiliza el método predict sobre los datos de prueba `(x_test)` y guárdalo en el arreglo `predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c09920-4a39-47d2-85c4-3df3253e0e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = LinearReg.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385a607-ad9c-4062-9c89-8e8ce6bc1ed3",
   "metadata": {},
   "source": [
    "Este es un método del modelo `LinearReg`. El método `predict` se usa para obtener predicciones del modelo basadas en nuevos datos que el modelo no ha visto durante el entrenamiento. En este caso, el método `predict` toma como entrada `x_test`, que son los datos de prueba o un conjunto de características nuevas para las cuales quieres predecir los valores de la variable objetivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc27af14-95d7-4755-9e9f-3ae2379c8b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test, y=predictions)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)  # Línea diagonal\n",
    "plt.xlabel('Valores Reales')\n",
    "plt.ylabel('Predicciones')\n",
    "plt.title('Valores Reales vs Predicciones')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e0b64-b8d7-44d5-9e34-4fade473347f",
   "metadata": {},
   "source": [
    "## 6. Evaluación del Modelo\n",
    "\n",
    "**A4) Usando las predicciones y el dataframe y_test, calcula el valor de cada métrica usando la función apropiada.**\n",
    "\n",
    "```python\n",
    "LinearRegression_MAE = np.mean(np.absolute(predictions-y_test))\n",
    "LinearRegression_MSE = np.mean((predictions-y_test)**2)\n",
    "LinearRegression_R2 = metrics.r2_score(y_pred=predictions, y_true=y_test)\n",
    "```\n",
    "\n",
    "Este bloque de código está calculando tres métricas estadísticas para evaluar el rendimiento de un modelo de regresión lineal basado en las predicciones realizadas y los valores reales (y_test).\n",
    "\n",
    "1. **MAE (Mean Absolute Error - Error Absoluto Medio):**\n",
    "   - `LinearRegression_MAE = np.mean(np.absolute(predictions-y_test))`\n",
    "   - El MAE es el promedio de los valores absolutos de los errores individuales entre las predicciones y los valores reales. Es una medida de la magnitud media de los errores en un conjunto de predicciones, sin considerar su dirección (ignora si son positivos o negativos).\n",
    "   - En este caso, se calcula como el promedio del valor absoluto de la diferencia entre las predicciones y los valores reales.\n",
    "\n",
    "2. **MSE (Mean Squared Error - Error Cuadrático Medio):**\n",
    "   - `LinearRegression_MSE = np.mean((predictions-y_test)**2)`\n",
    "   - El MSE es el promedio de los cuadrados de los errores; es decir, la diferencia entre el valor real y la predicción se eleva al cuadrado y luego se promedia sobre el conjunto de datos. Esta métrica penaliza más los errores grandes, lo que la hace muy útil cuando los errores grandes son particularmente indeseables.\n",
    "   - Se calcula como el promedio de los errores al cuadrado entre las predicciones y los valores reales.\n",
    "\n",
    "3. **R² (Coeficiente de Determinación):**\n",
    "   - `LinearRegression_R2 = metrics.r2_score(y_pred=predictions, y_true=y_test)`\n",
    "   - El coeficiente R² es una medida estadística que representa la proporción de la varianza para una variable dependiente que es predecible a partir de las variables independientes. Un valor de R² de 1 indica que el modelo puede predecir perfectamente los valores reales, mientras que un valor de 0 indica que el modelo no es mejor que simplemente tomar la media de los datos como predicción para todos los casos.\n",
    "   - En este código, se utiliza la función `r2_score` de la biblioteca de `metrics` para calcular R² basado en las predicciones y los valores reales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a74d9-f4d0-44d8-a7ca-27db15c033e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d774a-dacb-4500-9b85-10d0c630b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Report = pd.DataFrame({'MAE':LinearRegression_MAE, 'MSE': LinearRegression_MSE, 'R2': LinearRegression_R2}, range(1))\n",
    "Report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
